{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Accent Transfer Techniques for Speech Synthesis\n",
        "## Signal Minded\n",
        "\n",
        "\n",
        "*  Pradnesh Prasad Kalkar (190101103), B.Tech. Computer Science and Engineering\n",
        "*  Saket Kumar Singh (190101081), B.Tech. Computer Science and Engineering\n",
        "*  Anirudh Phukan (190101104), B.Tech. Computer Science and Engineering\n",
        "*  Mesharya M Choudhary (190101053), B.Tech. Computer Science and Engineering\n",
        "*  Korada Pavan Kumar (190102093), B.Tech. Electronics and Communication Engineering\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "dbrHcKmBHWsj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Table of Contents\n",
        "\n",
        "Overview\n",
        "\n",
        "*   Results and Conclusions\n",
        "*   Dataset Generation\n",
        "*   Neural Style Transfer\n",
        "*   Simulation settings\n",
        "*   Dynamic Time Warping\n",
        "*   Fine Tuning on Audio Representations\n",
        "*   Existing Work"
      ],
      "metadata": {
        "id": "NOM2IjjVJCPW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Overview\n",
        "\n",
        "---\n",
        "Note: This report highlights only the important code snippets. For the complete code kindly refer to [GitHub](https://github.com/sksingh1202/human-accent-transfer-learning).\n",
        "\n",
        "*   Data Generation\n",
        "  *   MelDatasetGen.py generates Mel Spectrograms for each of the audio files\n",
        "  *   SpecAugmentGen.py generates Mel Spectrograms with a data augmentation technique called SpecAugment applied on it for each of the audio files\n",
        "  *   WindowAugGen.py generates Spectrograms for each of the audio files with 4 different window sizes.\n",
        "  * preprocess.py is used to split **.mp3** audio files of Speech Accent Archive Dataset into 5 second **.wav** files.\n",
        "  * split.py is used to create train and validation split in a dataset containing files divided in classes \n",
        "*   Finetuning: FineTune.py is used to fine-tune the weights of pre-trained VGG-19 model under the 5 settings mentioned later in the section.\n",
        "*   dynamic_time_warping.py uses **Dynamic Time Warping** to synhronize audio signals as a preprocessing step to neural style transfer\n",
        "*   neural_style_transfer.py performs NST given two audios and the file does this using 6 different weights for VGG19, including the imagenet pretrained. The output is the spectrogram of the generated image in the png form. \n",
        "*   image_to_spectrogram.py takes a generated spectrogram image and converts it to an audio file.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "TvP1tUg4MdB2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  Results and Conclusions\n",
        "\n",
        "![cantonese](https://github.com/sksingh1202/human-accent-transfer-learning/blob/master/images/cantonese.png?raw=true)\n",
        "\n",
        "Cantonese\n",
        "\n",
        "![hindi](https://github.com/sksingh1202/human-accent-transfer-learning/blob/master/images/hindi.png?raw=true)\n",
        "\n",
        "Hindi\n",
        "\n",
        "![english](https://github.com/sksingh1202/human-accent-transfer-learning/blob/master/images/english.png?raw=true)\n",
        "\n",
        "English\n",
        "\n",
        "![dutch](https://github.com/sksingh1202/human-accent-transfer-learning/blob/master/images/dutch.png?raw=true)\n",
        "\n",
        "Dutch\n",
        "\n",
        "![russian](https://github.com/sksingh1202/human-accent-transfer-learning/blob/master/images/russian.png?raw=true)\n",
        "\n",
        "Russian\n",
        "\n",
        "\n",
        "Following conclusions were drawn based on the results obtained:\n",
        "\n",
        "*   The output audios obtained from fine-tuned models seemed to be a bit smoother compared to the pretrained model.\n",
        "*   The outputs of the fine-tuned model using Mel Spectogram performed to  be the best (qualitatively better than the pre-trained model).\n",
        "*   The outputs of the other fine-tuned models were not classified correctly using respective models.\n",
        "*   In all the cases the output of the models was noisy.\n",
        "*   The outputs generated with DTW as preprocessing step were found to be noisier.\n",
        "\n"
      ],
      "metadata": {
        "id": "KcuIS9Zxgi1E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset Generation\n",
        "\n",
        "In the below code snipped we randomly choose 10% of the samples for validation set.\n",
        "\n",
        "```\n",
        "N = len(filenames)\n",
        "# we want the validation set to contain 10% of the samples\n",
        "M = math.floor(N/10)\n",
        "for filename in filenames:\n",
        "    # randomly choose M files from the total N files\n",
        "    random_number = random.randint(0,N)\n",
        "    datasettype = \"\"\n",
        "    if random_number<=M:\n",
        "            datasettype = \"validation\"\n",
        "            N-=1\n",
        "            M-=1\n",
        "    else:\n",
        "            datasettype = \"training\"\n",
        "            N-=1\n",
        "```\n",
        "\n",
        "SpecAugment is a data augmentation technique for spectrograms which works by time and frequency masking.\n",
        "\n",
        "In the below code snippet number of mask bands, frequency mask size and time mask size are input parameters of the function. The size of frequency band and time band as well as the starting position of the bands is randomly chosen and the value in the locations covered by the band is set to 0.\n",
        "```\n",
        "# SpecAugment function which performs time,frquency masking over spectrograms for data augmentation\n",
        "def spec_augment(spec: np.ndarray, num_mask=2, \n",
        "                 freq_masking_max_percentage=0.15, time_masking_max_percentage=0.3):\n",
        "\n",
        "    spec = spec.copy()\n",
        "    for i in range(num_mask):\n",
        "        all_frames_num, all_freqs_num = spec.shape\n",
        "        freq_percentage = random.uniform(0.0, freq_masking_max_percentage)\n",
        "        \n",
        "        num_freqs_to_mask = int(freq_percentage * all_freqs_num)\n",
        "        f0 = np.random.uniform(low=0.0, high=all_freqs_num - num_freqs_to_mask)\n",
        "        f0 = int(f0)\n",
        "        spec[:, f0:f0 + num_freqs_to_mask] = 0\n",
        "\n",
        "        time_percentage = random.uniform(0.0, time_masking_max_percentage)\n",
        "        \n",
        "        num_frames_to_mask = int(time_percentage * all_frames_num)\n",
        "        t0 = np.random.uniform(low=0.0, high=all_frames_num - num_frames_to_mask)\n",
        "        t0 = int(t0)\n",
        "        spec[t0:t0 + num_frames_to_mask, :] = 0\n",
        "    \n",
        "    return spec\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "The below snippet is from preprocess.py that is used to split **.mp3** audio files from Speech Accent Archive Dataset into 5 second **.wav** files.\n",
        "\n",
        "```\n",
        "# Calculate the total number of samples for each clip\n",
        "clip_samples = int(clip_duration * sr)\n",
        "# Calculate the total number of clips that can be extracted from the audio signal\n",
        "num_clips = int(np.floor(len(y) / clip_samples))\n",
        "# Extract the clips and save them as WAV files\n",
        "for i in range(num_clips):\n",
        "    # Calculate the start and end sample indices for the current clip\n",
        "    start_sample = i * clip_samples\n",
        "    end_sample = (i + 1) * clip_samples\n",
        "    # Extract the current clip from the audio signal\n",
        "    clip = y[start_sample:end_sample]\n",
        "    clip = librosa.util.normalize(clip)\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "1ry5qh4LMgXu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Neural Style Transfer\n",
        "\n",
        "We take two audio files as input. Then, we compute the spectrogram from the audio. These spectrograms are then stored as images, which are then fed to the Neural Style Transfer (NST) model. In NST, the target image is initialized to the content image and then this target image is optimized using content and style loss. The optimized target image is then converted back to an audio signal for the final output using inverse stft. For representing content and style, we use the VGG-19 model, whose weights remain frozen throughout the NST optimization process. Let C, S and G be the content, style and the generated image respectively. G is initialized with the content image. The following cost function is used to optimize the target image G. \n",
        "\n",
        "The cost function minimizes both the style and the content cost. The formula is: \n",
        "\n",
        "$$J(G) = \\alpha J_{content}(C,G) + \\beta J_{style}(S,G)$$\n",
        "\n",
        "\n",
        "#### Content Cost Function\n",
        "One goal to aim for when performing NST is for the content in generated image G to match the content of image C. A method to achieve this is to calculate the content cost function, which will be defined as:\n",
        "\n",
        "$$J_{content}(C,G) =  \\frac{1}{4 \\times n_H \\times n_W \\times n_C}\\sum _{ \\text{all entries}} (a^{(C)} - a^{(G)})^2\\tag{1} $$\n",
        "\n",
        "* Here, $n_H, n_W$ and $n_C$ are the height, width and number of channels of the hidden layer chosen, and appear in a normalization term in the cost. \n",
        "* Note that $a^{(C)}$ and $a^{(G)}$ are the 3D volumes corresponding to a hidden layer's activations. \n",
        "\n",
        "#### Gram Matrix \n",
        "We will compute the Style matrix by multiplying the \"unrolled\" activations matrix with its transpose as shown. The result is a matrix of dimension $(n_C,n_C)$ where $n_C$ is the number of filters (channels). \n",
        "* The value $G_{(gram)i,j}$ measures the correlation between the filters corresponding to the $i$th and $j$th channel i.e. how much do these features occur together.\n",
        "* The diagonal elements $G_{(gram)ii}$ measure how \"active\" a filter $i$ is. \n",
        "* For example, suppose filter $i$ is detecting vertical textures in the image. Then $G_{(gram)ii}$ measures how common  vertical textures are in the image as a whole.\n",
        "* If $G_{(gram)ii}$ is large, this means that the image has a lot of vertical texture. \n",
        "* In this way, the gram matrix represents the style of the image.\n",
        "\n",
        "![gram_matrix](https://github.com/sksingh1202/human-accent-transfer-learning/blob/master/images/gram_matrix.png?raw=true)\n",
        "\n",
        "#### Style Cost Function\n",
        "Now, for the style cost we will minimize the distance between the Gram matrix of the \"style\" image S and the Gram matrix of the \"generated\" image G. \n",
        "* Consider only a single hidden layer with activations $a^{[l]}$.  \n",
        "* The corresponding style cost for this layer is defined as: \n",
        "\n",
        "$$J_{style}^{[l]}(S,G) = \\frac{1}{4 \\times {n_C}^2 \\times (n_H \\times n_W)^2} \\sum _{i=1}^{n_C}\\sum_{j=1}^{n_C}(G^{(S)}_{(gram)i,j} - G^{(G)}_{(gram)i,j})^2\\tag{2} $$\n",
        "\n",
        "* $G_{gram}^{(S)}$ Gram matrix of the \"style\" image.\n",
        "* $G_{gram}^{(G)}$ Gram matrix of the \"generated\" image.\n",
        "* We consider the weighted average of the style cost for 5 layers.\n",
        "\n",
        "For code, kindly refer the neural_style_transfer.py file in our github repository."
      ],
      "metadata": {
        "id": "R0pTvbrxB2l8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Simulation Settings\n",
        "\n",
        "### Transfer Learning\n",
        "Transfer learning was performed on all datasets mentioned in the previous section to generate 5 set of weights which are present in a folder named **weights** in [GitHub](https://github.com/sksingh1202/human-accent-transfer-learning). Following is the description of the files:\n",
        "\n",
        "*   **vgg19_fine_tuned.h5** - Fine-tuned weights by splitting the audio clips into 5 sec intervals using the [Speech Accent Dataset](https://www.kaggle.com/datasets/rtatman/speech-accent-archive).\n",
        "*   **vgg19_fine_tuned_spec_augment.h5** - Fine-tuned weights using spec-augmentation on mel spectograms.\n",
        "*   **vgg19_fine_tuned_mel.h5** - Fine-tuned weights using mel spectograms as input.\n",
        "*   **vgg19_fine_tuned_window_size.h5** - Fine-tuned weights using data-augmentation using different time windows to eliminate time-frequency tradeoff.\n",
        "*   **vgg19_fine_tuned_original_18.h5** - Fine-tuned weights using the original [Arctic Dataset](http://festvox.org/cmu_arctic/).\n",
        "\n",
        "\n",
        "### Neural style transfer experiment setup\n",
        "Following four configuration settings were used:\n",
        "\n",
        "\n",
        "*   Content weight: 1e-2, Style weight: 1\n",
        "*   Content weight: 1e-2, Style weight: 1e2\n",
        "*   Content weight: 1e-2, Style weight: 1e4\n",
        "*   Content weight: 1e-2, Style weight: 1e6\n",
        "\n",
        "Each of these configurations settings were used with VGG-19 with pre-trained weights and also the 5 weights generated as a result of fine-tuning as mentioned earlier. Moreover, every simulation involved either absence or presence of DTW as a pre-processing step.\n",
        "This gives us 4 * 6 * 2 = 48 different simulations."
      ],
      "metadata": {
        "id": "amnr38_fivRF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dynamic Time Warping\n",
        "\n",
        "DTW is an algorithm that aligns two time series data, useful for synchronizing audio files with variations in tempo. By converting audio files into Mel spectrograms, we can apply DTW to align the two representations, creating a synchronized version of the audio. DTW finds the optimal alignment by warping one or both time series while minimizing a distance measure between them, using dynamic programming\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "y1, fs = sf.read(\"./Dataset/american/arctic_a0001.wav\")\n",
        "y2, fs = sf.read(\"./Dataset/indian/arctic_a0001.wav\")\n",
        "# Add some simple padding\n",
        "i1 = P.argmax( y1 > P.sqrt((y1**2).mean())/3 )\n",
        "i2 = P.argmax( y2 > P.sqrt((y2**2).mean())/3 )\n",
        "I = max(i1, i2)*2\n",
        "z1 = y1[i1//5:(i1//5)*2]\n",
        "y1 = P.hstack([z1]*((I-i1)//len(z1)) + [z1[:((I - i1)%len(z1))]] + [y1])\n",
        "z2 = y2[i2//5:(i2//5)*2]\n",
        "y2 = P.hstack([z2]*((I-i2)//len(z2)) + [z2[:((I - i2)%len(z2))]] + [y2])\n",
        "print(\"Setting padding to {0:.2f} s\".format(I/fs))\n",
        "# Manually downsample by factor of 2\n",
        "fs = fs//2\n",
        "y1 = decimate(y1, 2, zero_phase=True)\n",
        "y2 = decimate(y2, 2, zero_phase=True)\n",
        "# Normalize loudness\n",
        "v1 = P.sqrt((y1**2).mean())\n",
        "v2 = P.sqrt((y2**2).mean())\n",
        "y1 = y1/v1*.03\n",
        "y2 = y2/v2*.03\n",
        "```\n",
        "\n",
        "This code block loads two audio files in WAV format and performs some preprocessing to synchronize them.\n",
        "\n",
        "Some padding is added to both audio files by finding the maximum amplitude and repeating it until the desired padding length is achieved. Next, the code down-samples the audio files by a factor of 2 using the **decimate** function to reduce the sample rate of the audio files. Finally, the loudness of the audio files is normalized.\n",
        "\n",
        "The preprocessing step, especially normalization, is important for DTW to work effectively since it compares the shape of the two audio files.\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "# Performing interpolation and warping\n",
        "wp = wp[::-1, :]\n",
        "y1_st, y1_end = wp[0, 0]*hop_size, wp[-1, 0]*hop_size\n",
        "y2_st, y2_end = wp[0, 1]*hop_size, wp[-1, 1]*hop_size\n",
        "y1 = y1[y1_st:y1_end]\n",
        "y2 = y2[y2_st:y2_end]\n",
        "wp[:, 0] = wp[:, 0] - wp[0,0]\n",
        "wp[:, 1] = wp[:, 1] - wp[0,1]\n",
        "wp_s = P.asarray(wp) * hop_size / fs\n",
        "i, I = P.argsort(wp_s[-1, :])\n",
        "x, y = close_points(\n",
        "  P.array([wp_s[:,i]/wp_s[-1,i], wp_s[:,I]/wp_s[-1,I]]), s=1)\n",
        "f = mono_interp(x, y, extrapolate=True)\n",
        "yc,yo = (y1,y2) if i==1 else (y2, y1)\n",
        "l_hop = 64\n",
        "stft = librosa.stft(yc, n_fft=512, hop_length=l_hop)\n",
        "z = len(yo)//l_hop + 1\n",
        "t = P.arange(0, 1, 1/z)\n",
        "time_steps = P.clip( f(t) * stft.shape[1], 0, None )\n",
        "# Performing vocoder warping\n",
        "warped_stft = variable_phase_vocoder(stft, time_steps, hop_length=l_hop)\n",
        "```\n",
        "\n",
        "The warping path **wp** is used to align the two audio files by cropping them and interpolating the time steps to create a synchronized version of the audio. The starting and ending indices for each audio file are calculated from **wp**, and then the warping path is modified so that it starts at 0 for both the x and y coordinates. The modified warping path is then sorted by the final y coordinate and used to interpolate the time steps between the two audio files using **mono_interp**.\n",
        "\n",
        "The interpolated time steps are then used to perform variable phase vocoding on the STFT (Short-Time Fourier Transform) of the audio files using **variable_phase_vocoder**. \n",
        "\n",
        "**Variable phase vocoding**(VPV) is a technique used for modifying the timing of audio signals without changing their pitch or spectral content. It involves dividing the audio signal into overlapping short-time windows and converting each window into its corresponding complex-valued STFT representation.\n",
        "\n"
      ],
      "metadata": {
        "id": "g5Esltc4SJAF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fine Tuning on Audio Representations\n",
        "\n",
        "By utilizing pre-trained neural network models on large datasets such as ImageNet, similar problems with new datasets can be solved efficiently. The pre-trained models provide a wealth of knowledge that can be leveraged, reducing the computation cost and training time on a new dataset.\n",
        "\n",
        "To implement this approach, the entire pre-trained model is first **frozen** and a new **classification layer** is added. The last layer is then trained for a few epochs to ensure its sensitivity. This is shown in the following code snippet:\n",
        "\n",
        "```\n",
        "base_model = tf.keras.applications.VGG19(include_top=False,\n",
        "                                  input_shape=IMG_SHAPE,\n",
        "                                  weights='imagenet')\n",
        "\n",
        "base_model.trainable = False\n",
        "\n",
        "global_average_layer = tf.keras.layers.GlobalAveragePooling2D()\n",
        "feature_batch_average = global_average_layer(feature_batch)\n",
        "\n",
        "prediction_layer = tf.keras.layers.Dense(len(class_names), activation='softmax')\n",
        "prediction_batch = prediction_layer(feature_batch_average)\n",
        "\n",
        "inputs = tf.keras.Input(shape=IMG_SHAPE)\n",
        "x = inputs\n",
        "x = preprocess_input(x)\n",
        "x = base_model(x, training=False)\n",
        "x = global_average_layer(x)\n",
        "print(x.shape)\n",
        "x = tf.keras.layers.Dropout(0.2)(x)\n",
        "print(x.shape)\n",
        "outputs = prediction_layer(x)\n",
        "print(x.shape)\n",
        "model = tf.keras.Model(inputs, outputs)\n",
        "\n",
        "base_learning_rate = 0.0001\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=base_learning_rate),\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "history = model.fit(train_dataset,\n",
        "      epochs=initial_epochs,\n",
        "      validation_data=validation_dataset)\n",
        "```\n",
        "\n",
        "\n",
        "Subsequently, all the layers are **unfrozen** and **trained together** with the new layer to fine-tune the entire model. This process allows the model to learn **task-specific** features while retaining the previously learned features, making it particularly useful when working with limited resources or small datasets. This can be seen in the following code snippet:\n",
        "\n",
        "```\n",
        "base_model.trainable = True\n",
        "\n",
        "model.compile(optimizer = tf.keras.optimizers.RMSprop(learning_rate=base_learning_rate/10),\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "fine_tune_epochs = 10\n",
        "total_epochs =  initial_epochs + fine_tune_epochs\n",
        "\n",
        "history_fine = model.fit(train_dataset,\n",
        "                         epochs=total_epochs,\n",
        "                         initial_epoch=history.epoch[-1],\n",
        "                         validation_data=validation_dataset)\n",
        "```\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "z-A5X7OsSISY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  Existing Work\n",
        "\n",
        "Reconstruction of historic artwork attracted many artists as well as scientists in transfer of style in images. Thus, there were initial works where style transfer without help of neural network like image-based artistic rendering which include stroke based (developing of strokes of new style on req. content) and region-based (segmenting the main image-divide and conquer). There were limitations like primitive transfer, narrow range of style etc which compelled to use NN. \n",
        "\n",
        "We want to transfer style in audio inputs for previously mentioned applications. There is very minimal material found on audio style transfer, which also use image style transfer techniques to audio properties. Spectrogram is an extremely useful tool for extracting property of time-varying frequency properties of audio signals. Thus, we apply convolutional neural networks (CNN’s) for the produced spectrogram images. \n",
        "\n",
        "Gatys et al. first introduced NST algorithm using CNN where he proposed that the content and style can be represented by internal layers.\n",
        "\n",
        "There are other methods where GAN is used for obtaining mixed style images like the work produced by Chen et al. which obtains cartoonized images using GAN. We didn't focus on this topic since training and experimenting with GAN would not be time-feasible with resources that we possess.\n",
        "\n",
        "We have used VGG19 as our CNN model since it's size is small enough to avoid difficulties in choosing which layers for content and style representations and is trainable with our resource constraints. \n",
        "\n",
        " \n"
      ],
      "metadata": {
        "id": "EF9W5APlidyk"
      }
    }
  ]
}